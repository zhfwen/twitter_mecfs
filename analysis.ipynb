{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Fetch Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import time\n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "\n",
    "# Authenticate to Twitter\n",
    "api_key = \"\"\n",
    "api_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "bearer_token = \"\"\n",
    "\n",
    "# V1 Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key=api_key, consumer_secret=api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# V2 Authentication\n",
    "client = tweepy.Client(\n",
    "    bearer_token,\n",
    "    api_key,\n",
    "    api_secret,\n",
    "    access_token,\n",
    "    access_token_secret,\n",
    "    wait_on_rate_limit=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Function to retrieve tweets for a specific time range with pagination\n",
    "\n",
    "def get_tweets_for_date_range(client, start_date, end_date, max_tweets):\n",
    "    # Convert dates to Twitter API-friendly string format\n",
    "    start_time = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_time = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    # Initialize variables\n",
    "    tweets_obtained = 0\n",
    "    total_tweets = []\n",
    "    next_token = None\n",
    "\n",
    "    # Continue fetching results until the desired limit is reached or there are no more results\n",
    "    while tweets_obtained < max_tweets:\n",
    "        # Calculate the remaining tweets to fetch in this iteration\n",
    "        remaining_tweets = min(max_tweets - tweets_obtained, 500)\n",
    "\n",
    "        # Perform the search with pagination\n",
    "        tweets = client.search_all_tweets(\n",
    "            query=\"(me/cfs OR Myalgic encephalomyelitis/chronic fatigue syndrome OR Myalgic encephalomyelitis OR chronic fatigue syndrome) lang:en -is:retweet\",\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            max_results=remaining_tweets,\n",
    "            next_token=next_token\n",
    "        )\n",
    "\n",
    "        # Update the counter for tweets obtained\n",
    "        tweets_obtained += len(tweets.data)\n",
    "        total_tweets.extend(tweets.data)\n",
    "\n",
    "        # Check for next_token\n",
    "        next_token = tweets.meta.get('next_token') if hasattr(tweets, 'meta') else None\n",
    "\n",
    "        # If there are no more results, break out of the loop\n",
    "        if not next_token:\n",
    "            break\n",
    "\n",
    "        # Introduce a wait time to avoid rate limits\n",
    "        time.sleep(5)\n",
    "\n",
    "    return total_tweets\n",
    "\n",
    "# Main logic to fetch tweets for multiple date ranges\n",
    "\n",
    "\n",
    "def fetch_tweets_for_date_ranges(client, date_ranges, max_tweets_per_month, output_directory):\n",
    "    for start_date, end_date in date_ranges:\n",
    "        # Initialize variables\n",
    "        current_date = start_date\n",
    "        total_tweets = []\n",
    "\n",
    "        # Iterate over each month within the date range\n",
    "        while current_date < end_date:\n",
    "            next_month = current_date + timedelta(days=30)  # Approximate one month\n",
    "\n",
    "            # Fetch tweets for the current month\n",
    "            tweets_for_month = get_tweets_for_date_range(client, current_date, next_month, max_tweets_per_month)\n",
    "            total_tweets.extend(tweets_for_month)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Month: {current_date.strftime('%Y-%m')}, Tweets Obtained: {len(tweets_for_month)}\")\n",
    "\n",
    "            # Move to the next month\n",
    "            current_date = next_month\n",
    "\n",
    "        # Save results to a CSV file\n",
    "        output_file = f\"{output_directory}/tweets_{start_date.strftime('%Y%m')}_to_{end_date.strftime('%Y%m')}.csv\"\n",
    "        df = pd.DataFrame([{'Tweet ID': tweet.id, 'Text': tweet.text} for tweet in total_tweets])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(total_tweets)} tweets to {output_file}\")\n",
    "\n",
    "\n",
    "# Define the date ranges\n",
    "all_date_ranges = [\n",
    "    (datetime(2007, 3, 1), datetime(2009, 3, 1)),\n",
    "    (datetime(2009, 4, 1), datetime(2011, 4, 1)),\n",
    "    (datetime(2011, 5, 1), datetime(2013, 5, 1)),\n",
    "    (datetime(2013, 6, 1), datetime(2015, 6, 1)),\n",
    "    (datetime(2015, 7, 1), datetime(2017, 7, 1)),\n",
    "    (datetime(2017, 8, 1), datetime(2019, 8, 1)),\n",
    "    (datetime(2021, 1, 1), datetime(2021, 10, 27)),\n",
    "    (datetime(2021, 10, 28), datetime(2024, 1, 30))\n",
    "]\n",
    "\n",
    "# Specify parameters\n",
    "max_tweets_per_month = 4100\n",
    "output_directory = os.path.join(\"datasets\", \"fetched_tweets\")\n",
    "\n",
    "# Uncomment the line below to fetch tweets after setting up your Twitter client\n",
    "# fetch_tweets_for_date_ranges(client, all_date_ranges, max_tweets_per_month, output_directory)\n",
    "\n",
    "def process_and_clean_tweets(file_paths, output_directory):\n",
    "    for file_path in file_paths:\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Apply the cleaning function to the 'Text' column and create a new column 'Cleaned Text'\n",
    "        # Generate cleaned file path\n",
    "        cleaned_file_name = f\"cleaned_{file_path.split('/')[-1]}\"\n",
    "        cleaned_csv_file_path = f\"{output_directory}/{cleaned_file_name}\"\n",
    "\n",
    "        # Save the cleaned DataFrame to a CSV file\n",
    "        df.to_csv(cleaned_csv_file_path, index=False)\n",
    "        print(f\"Cleaned tweets saved to {cleaned_csv_file_path}\")\n",
    "\n",
    "\n",
    "# Define the list of file paths and output directory\n",
    "file_paths = [\n",
    "    os.path.join(\"datasets\", \"fetched_tweets\", 'tweets_october28_2021_to_january30_2024.csv'),\n",
    "    os.path.join(\"datasets\", \"fetched_tweets\", 'tweets_jan_2010_to_dec11_2019.csv'),\n",
    "    os.path.join(\"datasets\", \"fetched_tweets\", 'tweets_august_2017_to_august_2019.csv')\n",
    "]\n",
    "output_directory = os.path.join(\"datasets\", \"raw\")\n",
    "\n",
    "# Process and clean tweets\n",
    "process_and_clean_tweets(file_paths, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Unzip Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "import os \n",
    "\n",
    "# Path to the zip file\n",
    "\n",
    "dir = os.path.join(\"datasets\", \"raw\")\n",
    "\n",
    "with py7zr.SevenZipFile(os.path.join(dir, \"datasets.7z\"), mode='r') as archive:\n",
    "    archive.extractall(path=dir)\n",
    "    print(f\"Files extracted to '{dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Generate Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from transformers import RobertaTokenizer\n",
    "from contractions import fix\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "import dask.dataframe as dd\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "num_workers = max(cpu_count() - 1, 1)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text)\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    text = text.replace(\"&amp;\", \"&\")      # Replace &amp; with &\n",
    "    text = text.replace(\"&lt;\", \"<\")       # Replace &lt; with <\n",
    "    text = text.replace(\"&gt;\", \">\")       # Replace &gt; with >\n",
    "    text = text.replace(\"&quot;\", \"\\\"\")    # Replace &quot; with \"\n",
    "    text = text.replace(\"&#39;\", \"'\")      # Replace &#39; with '\n",
    "    text = text.replace(\"&nbsp;\", \" \")     # Replace &nbsp; with a space\n",
    "    text = text.replace(\"&cent;\", \"¢\")     # Replace &cent; with ¢\n",
    "    text = text.replace(\"&pound;\", \"£\")    # Replace &pound; with £\n",
    "    text = text.replace(\"&yen;\", \"¥\")      # Replace &yen; with ¥\n",
    "    text = text.replace(\"&euro;\", \"€\")     # Replace &euro; with €\n",
    "    text = text.replace(\"&copy;\", \"©\")     # Replace &copy; with ©\n",
    "    text = text.replace(\"&reg;\", \"®\")      # Replace &reg; with ®\n",
    "    \n",
    "    text = text.replace('\\u00A0', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    # Normalize hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Expand contractions\n",
    "    text = fix(text)\n",
    "    return text\n",
    "    \n",
    "def roberta_process(text):\n",
    "    text = preprocess(text)\n",
    "    # Handle emojis\n",
    "    text = emoji.demojize(text)\n",
    "    # Strip extra whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def lexicon_preprocess(text):\n",
    "    text = roberta_process(text)\n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Retains only words and whitespace\n",
    "    return text\n",
    "\n",
    "def lda_process(text):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    # Helper function to map POS tags to WordNet's format\n",
    "    def get_wordnet_pos(word):\n",
    "        from nltk.corpus import wordnet\n",
    "        tag = pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    text = preprocess(text)\n",
    "    text = re.sub('<USER>', '', text)\n",
    "    text = text.replace('/', ' ')\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    terms = [\n",
    "        r\"me\",\n",
    "        r\"cfs\",\n",
    "        r\"cf\",\n",
    "        r\"me/cf\",\n",
    "        r\"me/cfs\",\n",
    "        r\"mecfs\",\n",
    "        r\"mecf\",\n",
    "        r\"myalgic encephalomyelitis/chronic fatigue syndrome\",\n",
    "        r\"myalgic encephalomyelitis\",\n",
    "        r\"chronic fatigue syndrome\",\n",
    "    ]\n",
    "    \n",
    "    pattern = r\"|\".join([rf\"\\b{term}\\b\" for term in terms])\n",
    "    text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(word)) \n",
    "        for word in words if word.isalnum()\n",
    "    ]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "global_id = 1\n",
    "\n",
    "for d in datasets:\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(os.path.join(\"datasets\", \"raw\", d))\n",
    "\n",
    "    # Drop rows where 'text' is NaN\n",
    "    df = df.dropna(subset=['Text'])\n",
    "    \n",
    "    # Apply the cleaning function\n",
    "    df['text'] = df['Text'].swifter.allow_dask_on_strings(enable=True).apply(roberta_process)\n",
    "    df['text_lexicon'] = df['Text'].swifter.allow_dask_on_strings(enable=True).apply(lexicon_preprocess)\n",
    "    df['text_lda'] = df['Text'].swifter.allow_dask_on_strings(enable=True).apply(lda_process)\n",
    "\n",
    "    # Remove rows where 'text' is empty or contains only whitespace\n",
    "    df = df[df['text'].str.strip().astype(bool)]\n",
    "\n",
    "    # Add the unique ID column\n",
    "    num_rows = len(df)\n",
    "    df['id'] = range(global_id, global_id + num_rows)\n",
    "\n",
    "    # Update the global ID counter\n",
    "    global_id += num_rows\n",
    "    \n",
    "    df = df[[\"id\", \"text\", \"text_lexicon\", \"text_lda\"]]\n",
    "    \n",
    "    columns_to_check = ['text', 'text_lexicon', 'text_lda']\n",
    "\n",
    "    for col in columns_to_check:\n",
    "        if col in df.columns:\n",
    "            na_count = df[col].isna().sum()\n",
    "            print(f\"{col}: {na_count} NaN values found in '{col}' column.\")\n",
    "        else:\n",
    "            print(f\"{col}: '{col}' column not found.\")\n",
    "\n",
    "    # Save the updated dataset back to CSV\n",
    "    df.to_csv(os.path.join(\"datasets\", d), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentiment Analysis\n",
    "## 2.1. RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline\n",
    "import os\n",
    "from torch.nn.functional import softmax\n",
    "import re\n",
    "\n",
    "os.makedirs(\"sentiment_results\", exist_ok=True)\n",
    "\n",
    "class FileDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        super().__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.df.at[i, 'text']\n",
    "\n",
    "    def getId(self, i):\n",
    "        return self.df.at[i, 'id']\n",
    "    \n",
    "# Ensure you have a GPU available\n",
    "device = torch.device(0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "]\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "classifier = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(os.path.join(\"datasets\", dataset))\n",
    "    dat = FileDataset(df)\n",
    "    \n",
    "    probability_negative = []\n",
    "    probability_neutral = []\n",
    "    probability_positive = []\n",
    "    \n",
    "    for i, row_preds in enumerate(tqdm(classifier(dat, batch_size=32),total=len(dat),disable=False)):\n",
    "        prob_dict = {pred['label']: pred['score'] for pred in row_preds}\n",
    "        probability_negative.append(prob_dict.get('LABEL_0', 0.0))  # Assuming LABEL_0 is negative\n",
    "        probability_neutral.append(prob_dict.get('LABEL_1', 0.0))   # Assuming LABEL_1 is neutral\n",
    "        probability_positive.append(prob_dict.get('LABEL_2', 0.0))  # Assuming LABEL_2 is positive\n",
    "\n",
    "    df['probability_negative'] = probability_negative\n",
    "    df['probability_neutral'] = probability_neutral\n",
    "    df['probability_positive'] = probability_positive\n",
    "    \n",
    "    df['sentiment'] = df[['probability_negative', 'probability_neutral', 'probability_positive']].idxmax(axis=1)\n",
    "    df['sentiment'] = df['sentiment'].map({\n",
    "        'probability_negative': 'negative',\n",
    "        'probability_neutral': 'neutral',\n",
    "        'probability_positive': 'positive'\n",
    "    })\n",
    "\n",
    "    # Save the result to a CSV\n",
    "    df.to_csv(os.path.join(\"sentiment_results\", f\"result-roberta-{dataset}\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Bing Liu Lexicon (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load the Bing Lexicon\n",
    "bing_lexicon = bing_lexicon = pd.read_csv(\"bing_liu_lexicon.txt\", sep='\\t')\n",
    "\n",
    "# Convert to a set for fast lookup\n",
    "positive_words = set(bing_lexicon[bing_lexicon['sentiment'] == 'positive']['word'])\n",
    "negative_words = set(bing_lexicon[bing_lexicon['sentiment'] == 'negative']['word'])\n",
    "\n",
    "\n",
    "def calculate_bing_sentiment(text):\n",
    "    \"\"\"Calculates sentiment using the Bing Lexicon.\"\"\"\n",
    "    words = text.split()\n",
    "    pos_count = sum(1 for word in words if word.lower() in positive_words)\n",
    "    neg_count = sum(1 for word in words if word.lower() in negative_words)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(os.path.join(\"datasets\", dataset))\n",
    "\n",
    "    # Calculate sentiment labels\n",
    "    df['sentiment_score'] = df['text_lexicon'].apply(calculate_bing_sentiment)\n",
    "    df['sentiment'] = df['sentiment_score'].apply(\n",
    "        lambda score: 'positive' if score > 0 else ('negative' if score < 0 else 'neutral')\n",
    "    )\n",
    "    # Save the updated DataFrame\n",
    "    df.to_csv(os.path.join(\"sentiment_results\", f\"result-bing-{dataset}\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import itertools\n",
    "\n",
    "\n",
    "os.makedirs(\"stat_results\", exist_ok=True)\n",
    "\n",
    "def calculate_sentiment_proportions(df, source):\n",
    "    timeframes = df['time_period'].unique()\n",
    "    pairwise_comparisons = list(itertools.combinations(timeframes, 2))\n",
    "    output_data = []\n",
    "\n",
    "    for timeframe in timeframes:\n",
    "        # Filter data for the current timeframe\n",
    "        timeframe_data = df[df['time_period'] == timeframe]\n",
    "        total_count = len(timeframe_data)\n",
    "\n",
    "        # Calculate sentiment counts and proportions\n",
    "        sentiment_counts = timeframe_data['sentiment'].value_counts(normalize=False)\n",
    "        sentiment_proportions = timeframe_data['sentiment'].value_counts(normalize=True)\n",
    "\n",
    "        n_positive = sentiment_counts.get('positive', 0)\n",
    "        n_neutral = sentiment_counts.get('neutral', 0)\n",
    "        n_negative = sentiment_counts.get('negative', 0)\n",
    "\n",
    "        proportion_positive = sentiment_proportions.get('positive', 0) * 100\n",
    "        proportion_neutral = sentiment_proportions.get('neutral', 0) * 100\n",
    "        proportion_negative = sentiment_proportions.get('negative', 0) * 100\n",
    "\n",
    "        # Append data to output table\n",
    "        output_data.append([\n",
    "            timeframe,\n",
    "            total_count,\n",
    "            f\"{n_positive} ({proportion_positive:.1f})\",\n",
    "            f\"{n_neutral} ({proportion_neutral:.1f})\",\n",
    "            f\"{n_negative} ({proportion_negative:.1f})\",\n",
    "        ])\n",
    "\n",
    "    # Perform chi-squared test for proportions across timeframes\n",
    "    contingency_table = pd.crosstab(df['time_period'], df['sentiment'])\n",
    "    _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    print(f\"Total Chi^2 P-value: {p_value}\")\n",
    "\n",
    "    for c1, c2 in pairwise_comparisons:\n",
    "        contingency_table = pd.crosstab(df[df['time_period'].isin([c1, c2])]['time_period'], df['sentiment'])\n",
    "        _, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        print(f\"{c1} vs. {c2} Chi^2 P-value: {p_value}\")\n",
    "\n",
    "    # Append total row\n",
    "    total_count = len(df)\n",
    "    total_counts = df['sentiment'].value_counts(normalize=False)\n",
    "    total_proportions = df['sentiment'].value_counts(normalize=True)\n",
    "\n",
    "    total_positive = total_counts.get('positive', 0)\n",
    "    total_neutral = total_counts.get('neutral', 0)\n",
    "    total_negative = total_counts.get('negative', 0)\n",
    "\n",
    "    proportion_positive = total_proportions.get('positive', 0) * 100\n",
    "    proportion_neutral = total_proportions.get('neutral', 0) * 100\n",
    "    proportion_negative = total_proportions.get('negative', 0) * 100\n",
    "\n",
    "    output_data.append([\n",
    "        'Total',\n",
    "        total_count,\n",
    "        f\"{total_positive} ({proportion_positive:.1f})\",\n",
    "        f\"{total_neutral} ({proportion_neutral:.1f})\",\n",
    "        f\"{total_negative} ({proportion_negative:.1f})\",\n",
    "    ])\n",
    "\n",
    "    return pd.DataFrame(output_data, columns=[\"Timeframe\", \"N Tweets\", \"N Positive (%)\", \"N Neutral (%)\", \"N Negative (%)\"])\n",
    "\n",
    "\n",
    "def compare_sentiment(df_roberta, df_bing):\n",
    "    results = []\n",
    "\n",
    "    # Timeframe-specific comparisons\n",
    "    timeframes = df_roberta['time_period'].unique()\n",
    "    for timeframe in timeframes:\n",
    "        # Filter data for the current timeframe\n",
    "        roberta_timeframe = df_roberta[df_roberta['time_period'] == timeframe]\n",
    "        bing_timeframe = df_bing[df_bing['time_period'] == timeframe]\n",
    "\n",
    "        # Merge the two datasets on 'id'\n",
    "        merged = pd.merge(roberta_timeframe, bing_timeframe, on='id', suffixes=('_roberta', '_bing'))\n",
    "\n",
    "        # McNemar's Test\n",
    "        contingency = pd.crosstab(merged['sentiment_roberta'], merged['sentiment_bing'])\n",
    "        mcnemar_result = mcnemar(contingency, exact=False)\n",
    "\n",
    "        # Cohen's Kappa\n",
    "        kappa_score = cohen_kappa_score(merged['sentiment_roberta'], merged['sentiment_bing'])\n",
    "\n",
    "        results.append([timeframe, mcnemar_result.pvalue, kappa_score])\n",
    "\n",
    "    # Overall comparison\n",
    "    # Merge the datasets on 'id'\n",
    "    merged = pd.merge(df_roberta, df_bing, on='id', suffixes=('_roberta', '_bing'))\n",
    "\n",
    "    # McNemar's Test\n",
    "    contingency = pd.crosstab(merged['sentiment_roberta'], merged['sentiment_bing'])\n",
    "    mcnemar_result = mcnemar(contingency, exact=False)\n",
    "\n",
    "    # Cohen's Kappa\n",
    "    kappa_score = cohen_kappa_score(merged['sentiment_roberta'], merged['sentiment_bing'])\n",
    "\n",
    "    results.append(['Overall', mcnemar_result.pvalue, kappa_score])\n",
    "\n",
    "    # Return results as a DataFrame\n",
    "    return pd.DataFrame(results, columns=['Timeframe', \"McNemar's P-value\", \"Cohen's Kappa\"])\n",
    "\n",
    "# Datasets\n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "]\n",
    "\n",
    "# Process each dataset\n",
    "for dataset in datasets:\n",
    "    df_roberta_path = os.path.join(\"results\", f\"result-roberta-{dataset}\")\n",
    "    df_bing_path = os.path.join(\"results\", f\"result-bing-{dataset}\")\n",
    "\n",
    "    # Load datasets\n",
    "    df_roberta = pd.read_csv(df_roberta_path)\n",
    "    df_roberta = df_roberta[['id', 'sentiment']]\n",
    "    df_bing = pd.read_csv(df_bing_path)[['id', 'sentiment']]\n",
    "    df_bing = df_bing[['id', 'sentiment']]\n",
    "\n",
    "    # Add time period information\n",
    "    df_roberta['time_period'] = dataset\n",
    "    df_bing['time_period'] = dataset\n",
    "\n",
    "    # Concatenate for combined analysis\n",
    "    df_roberta['source'] = 'roberta'\n",
    "    df_bing['source'] = 'bing'\n",
    "\n",
    "    if 'df_r' not in locals():\n",
    "        df_r = df_roberta.copy()\n",
    "    else:\n",
    "        df_r = pd.concat([df_r, df_roberta], ignore_index=True)\n",
    "    if 'df_b' not in locals():\n",
    "        df_b = df_bing.copy()\n",
    "    else:\n",
    "        df_b = pd.concat([df_b, df_bing], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"RoBERTa\")\n",
    "df_roberta_output = calculate_sentiment_proportions(df_r, \"roberta\")\n",
    "display(df_roberta_output)\n",
    "df_roberta_output.to_csv(os.path.join(\"stat_results\", \"roberta.csv\"), index=False)\n",
    "\n",
    "print(\"Bing\")\n",
    "df_bing_output = calculate_sentiment_proportions(df_b, \"bing\")\n",
    "display(df_bing_output)\n",
    "df_bing_output.to_csv(os.path.join(\"stat_results\", \"bing.csv\"), index=False)\n",
    "\n",
    "print(\"RoBERTa vs. Bing\")\n",
    "# Perform comparison by timeframe\n",
    "timeframe_comparison = compare_sentiment(df_r, df_b)\n",
    "display(timeframe_comparison)\n",
    "timeframe_comparison.to_csv(os.path.join(\"stat_results\", \"roberta_vs_bing.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SHAP Explanation\n",
    "## 4.1 Chunk Dataset\n",
    "For parallel processing of SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List of dataset file names\n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "]\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Function to split and save datasets\n",
    "i_chunk = 0\n",
    "\n",
    "os.makedirs(os.path.join(\"datasets\", \"chunked\"), exist_ok=True)\n",
    "\n",
    "# Apply the function to each dataset\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(os.path.join(\"datasets\", dataset))\n",
    "\n",
    "    # Determine number of chunks\n",
    "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Get the chunk\n",
    "        chunk = df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "\n",
    "        # Define the output file name\n",
    "        output_file = f\"chunk_{i_chunk}_{dataset}\"\n",
    "\n",
    "        # Save the chunk to a new file\n",
    "        chunk.to_csv(os.path.join(\"datasets\", \"chunked\", output_file), index=False)\n",
    "        i_chunk += 1\n",
    "        print(f\"Saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Calculate SHAP Values in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapley\n",
    "os.makedirs(\"shap_results\", exist_ok=True)\n",
    "for i in range(i_chunk):\n",
    "    shapley.main(i, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Combining Chunked Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from get_shap_scores import get_shap_scores\n",
    "from multiprocess import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_workers = max(cpu_count() - 1, 1)\n",
    "\n",
    "    datasets = [\n",
    "        \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "        \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "        \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "    ]\n",
    "\n",
    "    shap_files = {}\n",
    "\n",
    "    args = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        shap_files[dataset] = []\n",
    "        for chunk in range(92):\n",
    "            fpath = os.path.join(\"shap_results\", f\"sv_probs_chunk_{chunk}_{dataset}.pkl\")\n",
    "            if os.path.exists(fpath):\n",
    "                df = pd.read_csv(os.path.join(\"datasets\", \"chunked\", f\"chunk_{chunk}_{dataset}\"))\n",
    "                id_min = df['id'].min()\n",
    "                id_max = df['id'].max()\n",
    "\n",
    "                for lb in range(3):\n",
    "                    args.append((fpath, lb, dataset, id_min, id_max))\n",
    "\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        results = list(tqdm(pool.imap(get_shap_scores, args), total=len(args)))\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"dataset\", \"label\", \"feature\", \"value\", \"base_value\", \"id\"])\n",
    "\n",
    "    for result in tqdm(results):\n",
    "        features, values, base_values, ids, label, dataset = result\n",
    "\n",
    "        df_temp = pd.DataFrame({\n",
    "            \"feature\": features,\n",
    "            \"value\": values,\n",
    "            \"base_value\": base_values,\n",
    "            \"id\": ids\n",
    "        })\n",
    "\n",
    "        df_temp[\"label\"] = label\n",
    "        df_temp[\"dataset\"] = dataset\n",
    "\n",
    "        # Append to the main dataframe\n",
    "        df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    # raise Exception()\n",
    "    # Save or return the dataframe (for further processing)\n",
    "    df.to_csv(os.path.join(\"shap_results\", \"sv_probs_raw.csv\"), index=False)\n",
    "    display(df.head())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Process Shap Values and Generate Figures and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(\"shap_figures\", exist_ok=True)\n",
    "formats = [\"pdf\", \"png\", \"tiff\"]\n",
    "for f in formats:\n",
    "    os.makedirs(os.path.join(\"shap_figures\", f), exist_ok=True)\n",
    "\n",
    "label_mapping = {\n",
    "    0: \"Negative\",\n",
    "    1: \"Neutral\",\n",
    "    2: \"Positive\"\n",
    "}\n",
    "\n",
    "time_mapping = {\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\": \"T1\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\": \"T2\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\": \"T3\",\n",
    "    \"total\": \"All\"\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "]\n",
    "\n",
    "df_sv = pd.read_csv(os.path.join(\"shap_results\", \"sv_probs_raw.csv\"))\n",
    "\n",
    "# This will collect the CI table rows\n",
    "ci_rows = []\n",
    "\n",
    "output = pd.DataFrame(columns=[\n",
    "    \"Dataset\", \"Time Period\", \"Sentiment\", \"Occurrence Threshold\",\n",
    "    \"N Features\", \"N Unique Features\", \"N Unique Features with >= X Occurrence\",\n",
    "    \"Feature Type\", \"Figure Name\"\n",
    "])\n",
    "\n",
    "for dataset in datasets + [\"total\"]:\n",
    "    if dataset != \"total\":\n",
    "        df_d = df_sv[df_sv[\"dataset\"] == dataset].copy()\n",
    "    else:\n",
    "        df_d = df_sv.copy()\n",
    "\n",
    "    df_d['feature'] = df_d['feature'].str.replace(\"Ġ\", \"\", regex=False).str.lower().str.strip()\n",
    "\n",
    "    for lb in range(3):\n",
    "        df_d_lb = df_d[df_d[\"label\"] == lb]\n",
    "        feature_counts = df_d_lb['feature'].value_counts()\n",
    "\n",
    "        for occurrence in [1, 100, 1000]:\n",
    "            df_d_lb_o = df_d_lb[df_d_lb['feature'].isin(feature_counts[feature_counts >= occurrence].index)]\n",
    "\n",
    "            # Group stats for sorting\n",
    "            grouped = df_d_lb_o.groupby('feature')['value']\n",
    "            for ascending, feature_type in [(False, \"Increase\"), (True, \"Decrease\")]:\n",
    "                stats_mean = grouped.mean().sort_values(ascending=ascending)\n",
    "                top_features = stats_mean.head(15).index\n",
    "\n",
    "                # Filter data for the top features\n",
    "                tf_data = df_d_lb_o[df_d_lb_o['feature'].isin(top_features)]\n",
    "\n",
    "                # Compute per-feature statistics\n",
    "                for feat in top_features:\n",
    "                    vals = tf_data.loc[tf_data['feature'] == feat, 'value']\n",
    "                    n = len(vals)\n",
    "                    mean = vals.mean()\n",
    "                    sem = vals.std(ddof=1) / np.sqrt(n)\n",
    "                    ci = sem * stats.t.ppf(0.975, df=n-1)  # 95% t-interval\n",
    "                    ci_rows.append({\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Time Period\": time_mapping[dataset],\n",
    "                        \"Sentiment\": label_mapping[lb],\n",
    "                        \"Occurrence\": occurrence,\n",
    "                        \"Feature\": feat,\n",
    "                        \"Mean_SHAP\": mean,\n",
    "                        \"CI_Lower\": mean - ci,\n",
    "                        \"CI_Upper\": mean + ci,\n",
    "                        \"Feature_Type\": feature_type\n",
    "                    })\n",
    "\n",
    "                # Prepare for plotting\n",
    "                sample_sizes = tf_data.groupby('feature')['value'].count()\n",
    "                tf_data['feature_with_n'] = tf_data['feature'].apply(\n",
    "                    lambda x: f\"{x} (n={sample_sizes[x]})\"\n",
    "                )\n",
    "                stats_order = stats_mean.loc[top_features]\n",
    "                colors = ['#ff5555' if v > 0 else '#55aaff' for v in stats_order]\n",
    "                order = [f\"{f} (n={sample_sizes[f]})\" for f in stats_order.index]\n",
    "\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.barplot(\n",
    "                    data=tf_data,\n",
    "                    x='value',\n",
    "                    y='feature_with_n',\n",
    "                    palette=colors,\n",
    "                    orient='h',\n",
    "                    order=order,\n",
    "                    err_kws={'linewidth': 1}\n",
    "                )\n",
    "                plt.xlabel('Mean SHAP Value')\n",
    "                plt.ylabel('Token Feature')\n",
    "                plt.tight_layout()\n",
    "\n",
    "                fig_name = f\"Mean_{time_mapping[dataset]}_{label_mapping[lb]}_{feature_type}_{occurrence}\"\n",
    "                for f in formats:\n",
    "                    plt.savefig(\n",
    "                        os.path.join(\"shap_figures\", f, f\"{fig_name}.{f}\"),\n",
    "                        dpi=300,\n",
    "                        bbox_inches='tight'\n",
    "                    )\n",
    "                plt.close()\n",
    "\n",
    "                # Record metadata\n",
    "                output.loc[len(output)] = {\n",
    "                    \"Dataset\": dataset,\n",
    "                    \"Time Period\": time_mapping[dataset],\n",
    "                    \"Sentiment\": label_mapping[lb],\n",
    "                    \"Occurrence Threshold\": occurrence,\n",
    "                    \"N Features\": len(df_d),\n",
    "                    \"N Unique Features\": len(df_d['feature'].unique()),\n",
    "                    \"N Unique Features with >= X Occurrence\": len(df_d_lb_o['feature'].unique()),\n",
    "                    \"Feature Type\": feature_type,\n",
    "                    \"Figure Name\": fig_name\n",
    "                }\n",
    "\n",
    "# Save the figure description\n",
    "output.to_csv(os.path.join(\"shap_figures\", \"shap_figure_description.csv\"), index=False)\n",
    "\n",
    "# Build and save the CI table\n",
    "ci_df = pd.DataFrame(ci_rows)\n",
    "ci_df.to_csv(os.path.join(\"shap_figures\", \"shap_values_with_95CI.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LDA Topic Modelling\n",
    "## 5.1. Generate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "num_workers = max(cpu_count() - 1, 1)\n",
    "\n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\"\n",
    "]\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "os.makedirs(\"lda_results\", exist_ok=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    return word_tokenize(text.lower())  # Example: simple tokenization\n",
    "\n",
    "\n",
    "dataframes = [pd.read_csv(os.path.join(\"datasets\", dataset)) for dataset in datasets]\n",
    "df_all = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "for dataset in datasets + ['total']:\n",
    "    if dataset == 'total':\n",
    "        df = df_all.copy()\n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(\"datasets\", dataset))\n",
    "    \n",
    "    tokenized_words = [preprocess(str(tweet)) for tweet in df['text_lda']]\n",
    "\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(tokenized_words)\n",
    "    corpus = [dictionary.doc2bow(tweet) for tweet in tokenized_words]\n",
    "\n",
    "    # Train LDA model\n",
    "    num_topics = 6\n",
    "    lda_model = gensim.models.LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=10,\n",
    "        workers=num_workers\n",
    "    )\n",
    "    \n",
    "    with open(os.path.join(\"lda_results\", f\"lda_model_{dataset}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(lda_model, f)\n",
    "\n",
    "    # Display topics\n",
    "    for idx, topic in lda_model.print_topics(num_words=5):\n",
    "        print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Generate Figures and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(\"lda_figures\", exist_ok=True)\n",
    "formats = [\"pdf\", \"png\", \"tiff\"]\n",
    "for f in formats:\n",
    "    os.makedirs(os.path.join(\"lda_figures\", f), exist_ok=True)\n",
    "\n",
    "datasets = [\n",
    "    \"cleaned_tweets_jan_2010_to_dec11_2019.csv\",\n",
    "    \"cleaned_tweets_dec12_2019_to_oct27_2021.csv\",\n",
    "    \"cleaned_tweets_october28_2021_to_january30_2024.csv\",\n",
    "    \"total\"\n",
    "]\n",
    "\n",
    "# Will collect rows for the CSV\n",
    "rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Load the fitted LDA model\n",
    "    with open(os.path.join(\"lda_results\", f\"lda_model_{dataset}.pkl\"), \"rb\") as fp:\n",
    "        lda_model = pickle.load(fp)\n",
    "\n",
    "    # Extract topics: list of (topic_idx, [(word, weight), ...])\n",
    "    topics = lda_model.show_topics(num_topics=-1, num_words=15, formatted=False)\n",
    "\n",
    "    for idx, topic in topics:\n",
    "        words, weights = zip(*topic)\n",
    "        # Append each word/weight to rows\n",
    "        for word, weight in zip(words, weights):\n",
    "            rows.append({\n",
    "                \"Dataset\": dataset,\n",
    "                \"Topic\": idx + 1,\n",
    "                \"Word\": word,\n",
    "                \"Weight\": weight\n",
    "            })\n",
    "\n",
    "        # Plot the bar chart\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x=list(weights), y=list(words), orient=\"h\")\n",
    "        plt.xlabel(\"Weight\")\n",
    "        plt.ylabel(\"Word\")\n",
    "        plt.title(f\"{dataset} — Topic {idx+1}\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save in all formats\n",
    "        for fmt in formats:\n",
    "            plt.savefig(\n",
    "                os.path.join(\"lda_figures\", fmt, f\"{dataset}_topic{idx+1}.{fmt}\"),\n",
    "                dpi=300,\n",
    "                bbox_inches=\"tight\"\n",
    "            )\n",
    "        plt.close()\n",
    "\n",
    "# Build DataFrame and save CSV\n",
    "lda_df = pd.DataFrame(rows)\n",
    "lda_df.to_csv(os.path.join(\"lda_figures\", \"lda_topic_word_weights.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiru-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
